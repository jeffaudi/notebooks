{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffaudi/notebooks/blob/main/Aircraft_Detection_with_IceVision_and_Airbus_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFcMPCoysXUH"
      },
      "source": [
        "## Testing various models for aircraft detection on Airbus imagery\n",
        "\n",
        "In recent years, artificial intelligence has made great strides in the field of computer vision. One area that has seen particularly impressive progress is object detection, with a variety of deep learning models achieving high levels of accuracy. However, this abundance of choice can be overwhelming for practitioners who are looking to implement an object detection system.\n",
        "\n",
        "On top of this, most public models and academic research are benchmarked on COCO which are dataset made of photographs. Satellite images are quite different from photographs: the objects to detect are usually much smaller and much more numerous, they are oriented in all kind of direction and acquired in slightly different colors. In photographs, trees are always seen as green objects with the trunk below the foliage. But not in aerial or satellite images.\n",
        "\n",
        "So, if a model architecture performs well on a photographic dataset, it does not mean that it will perform as well on an aerial dataset. And finding vehicles, wind turbines, buildings, roads, floods or crops are very different tasks. How can one find which model will work best for their data and application? In many cases, it is necessary to experiment with a few different models before finding the one that gives the best results for the specific task.\n",
        "\n",
        "How can we test many architectures?\n",
        "In this notebook, we will use the open source IceVision framework to experiment with different model architectures and compare their performance on a dataset representing the desired task i.e. finding aircrafts in satellite imagery.\n",
        "\n",
        "This notebook was inpired by: https://www.kaggle.com/code/aninda/icevision/notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D26Lw-1IsXUT"
      },
      "source": [
        "## Install IceVision\n",
        "\n",
        "IceVision is an agnostic computer vision framework which integrates hundreds of high-quality code source and pre-trained models from Torchvision, OpenMMLabs, Ultralytics YOLOv5 and Ross Wightmanâ€™s EfficientDet. It enables a end-to-end deep learning workflow by offering a unique interface to robust high-performance libraries like Pytorch Lightning and Fastai\n",
        "\n",
        "IceVision Unique Features:\n",
        "\n",
        "- Data curation/cleaning with auto-fix\n",
        "- Access to an exploratory data analysis dashboard\n",
        "- Pluggable transforms for better model generalization\n",
        "- Access to hundreds of neural net models\n",
        "- Access to multiple training loop libraries\n",
        "- Multi-task training to efficiently combine object detection, segmentation, and classification models\n",
        "\n",
        "Here, we will see how we can easily compare multiple models and backbones on the same dataset and task.\n",
        "\n",
        "IceVision provides a installation scripts which takes care of installing various libraries. Take care of getting the most up-to-date versions or versions adapted to your specific hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-2t3YF6cSJb"
      },
      "outputs": [],
      "source": [
        "# to correct 'distutils has no attribute version' error due to incompatible torch version.\n",
        "!pip install setuptools==59.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ytjwnOisXVf"
      },
      "outputs": [],
      "source": [
        "# Torch - Torchvision - IceVision - IceData - MMDetection - YOLOv5 - EfficientDet Installation\n",
        "!wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh\n",
        "\n",
        "# Choose your installation target: cuda11 or cuda10 or cpu\n",
        "!bash icevision_install.sh cuda11 master #> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb70htiSsXVq"
      },
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import os\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import glob\n",
        "import random\n",
        "import PIL\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "def clear_pyplot_memory():\n",
        "    plt.clf()\n",
        "    plt.cla()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLb3exZtsXVp"
      },
      "outputs": [],
      "source": [
        "# Here we import all functions from IceVision (similar to fastai)\n",
        "# This enable to redefine (and improve) various standard python function\n",
        "# If this fails, just re-run the cell\n",
        "from icevision.all import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAeYK1JGRx6v"
      },
      "source": [
        "Here, we retreive and display the versions for PyTorch, Torch Vision and Torch Ligthning. This is useful when using various environment like Google Cloud, Google Collab or Kaggle. All these environment do not have the lastest version of our favorite deep learning packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m2Vu6SisXVg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# Check PyTorch version\n",
        "print(f\"      Torch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"  Lightning version: {pl.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxphlL43sXVh"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecsq6SQnsXVr"
      },
      "source": [
        "### Deterministic behavior\n",
        "\n",
        "In this notebook, we are going to compare various deep learning architectures. So we are going to run the same training multiple times and compare the results (i.e. the final loss and final metric). In order for these comparaisons to be fair, we need to make sure that our training is deterministic. So we want to make sure that all random numbers used in functions (such as splitting between train and valid dataset) always return the same value. I place the function at the beggining of many cells so that I can replay them with no. But globally at the begining of the notebook should be enough if playing the cells only once.\n",
        "\n",
        "This function is pretty simple so it will not seed the workers. So we will only use the main thread. We will loose in speed but this is not an issue now because we just want to figure which is the best architecture for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OQ7FLRJsXVy"
      },
      "outputs": [],
      "source": [
        "def seed_everything(s=42):\n",
        "    random.seed(s)\n",
        "    os.environ['PYTHONHASHSEED'] = str(s)\n",
        "    np.random.seed(s)\n",
        "    torch.manual_seed(s)\n",
        "    #imgaug.random.seed(s)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(s)\n",
        "        torch.cuda.manual_seed_all(s)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "SEED = 42\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbw_xi47N_Vz"
      },
      "source": [
        "## Explore the dataset and Pleiades images\n",
        "\n",
        "For this article, we will use the [Airbus Aircraft Sample Dataset](https://www.kaggle.com/datasets/airbusgeo/airbus-aircrafts-sample-dataset) which was published on Kaggle in 2020. Keep in mind that this is a sample dataset i.e. it is too small to make a real benchmark. Also, it only contains commercial airports and commercial aircrafts. A real benchmark would need more objects and a mix of objects adapted to the business needs. The objective here is only to show the methodology so that you could run this with a real benchmark dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUiH0ullFqOq"
      },
      "outputs": [],
      "source": [
        "# download the Airbus Aircraft sample dataset\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "def download_file(url):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    # NOTE the stream=True parameter below\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192): \n",
        "                # If you have chunk encoded response uncomment if\n",
        "                # and set chunk_size parameter to None.\n",
        "                #if chunk: \n",
        "                f.write(chunk)\n",
        "    return local_filename\n",
        "\n",
        "DATA_DIR = Path('./airbus-aircrafts-sample-dataset')\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    \n",
        "    # download file from faudi.net\n",
        "    url = 'https://storage.googleapis.com/dl4eo-public/airbus/airbus-aircrafts-sample-dataset.zip'\n",
        "    filename = download_file(url)\n",
        "\n",
        "    # create a ZipFile object\n",
        "    zipobj = zipfile.ZipFile(filename, 'r')\n",
        "\n",
        "    # extract all the contents of the zip file in the current directory\n",
        "    zipobj.extractall(DATA_DIR)\n",
        "\n",
        "    # close the file\n",
        "    zipobj.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VThiRPZsXVz"
      },
      "source": [
        "## Explore images\n",
        "\n",
        "Let us first explore the content of the `image` directory. There are **JPEG** extracts of **Airbus Pleiades** imagery. We will display one of these images to check its size.\n",
        "\n",
        "Looking at satellite images of airports can be pretty fascinating. You can see all of the different buildings and airstrips and get a sense of how busy the airport is. But did you know that you can also learn a lot about the activity of an airport by automaticaly monitoring the location and type of aircrafts?\n",
        "\n",
        "*Note: if you get a `module 'PIL.TiffTags' has no attribute 'IFD'` error here, it might be a Google Colab glitch. Just restart the runtime and rerun the cells without reinstalling all the packages.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv8CZq3msXVz"
      },
      "outputs": [],
      "source": [
        "img_list = list(DATA_DIR.glob('images/*.jpg'))\n",
        "pickone = random.choice(img_list)\n",
        "img = PIL.Image.open(pickone)\n",
        "display(img)\n",
        "print(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP82lLp7sXV6"
      },
      "source": [
        "Thev images available in the dataset are too large to fit into GPU memory at full resolution. We do not want to resample them because we do not want to loose small details in the images. So the followinng code cut the 2560x2560 images into 512x512 tiles. We select pretty small tiles because we want to be able to fit more than one on the GPU to maintain a reasonable `batch_size`. But we also make sure that the size is large enough to fir most planes. In our case, 512 x 0.5m is 128 meters which is more than most planes wingspan.\n",
        "\n",
        "## Explore annotations\n",
        "\n",
        "Next, let's explore the annotations file `annotations.csv`. It contains a list of all the aircrafts visible on the images with the id of the associated image, a list of coordinates describing the outer boundaries of the aircraft and a label, usually `Aircraft` and sometimes `Truncated_aircraft`.\n",
        "\n",
        "Here we will convert the geometries to bounding boxes as this is the usual format for detection models and replace the two categories by only `aircraft`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_NQUEO9sXV7"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "# convert a string record into a valid python object\n",
        "def f(x): \n",
        "    return ast.literal_eval(x.rstrip('\\r\\n'))\n",
        "\n",
        "df = pd.read_csv(DATA_DIR / \"annotations.csv\", \\\n",
        "            converters={'geometry': f, 'class': lambda l: \"aircraft\"})\n",
        "df.rename(columns = {'class' : 'label'}, inplace = True)\n",
        "\n",
        "def getBounds(geometry):\n",
        "    try: \n",
        "        arr = np.array(geometry).T\n",
        "        xmin = np.min(arr[0])\n",
        "        ymin = np.min(arr[1])\n",
        "        xmax = np.max(arr[0])\n",
        "        ymax = np.max(arr[1])\n",
        "        return (xmin, ymin, xmax, ymax)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def getWidth(bounds):\n",
        "    try: \n",
        "        (xmin, ymin, xmax, ymax) = bounds\n",
        "        return np.abs(xmax - xmin)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def getHeight(bounds):\n",
        "    try: \n",
        "        (xmin, ymin, xmax, ymax) = bounds\n",
        "        return np.abs(ymax - ymin)\n",
        "    except:\n",
        "        return np.nan\n",
        "    \n",
        "# Create bounds, width and height\n",
        "df.loc[:,'bounds'] = df.loc[:,'geometry'].apply(getBounds)\n",
        "df.loc[:,'width'] = df.loc[:,'bounds'].apply(getWidth)\n",
        "df.loc[:,'height'] = df.loc[:,'bounds'].apply(getHeight)\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_D-zbwesXV7"
      },
      "source": [
        "We subsequently process the annotation DataFrame by associating each annotation with its tile rather than its imagery.\n",
        "\n",
        "*Note: here we do not use the parameter to provide an overlap between the tiles. This is because the train/val split is done by IceVision on the tiles and not on the source imagery as described on my other <a href=\"https://www.kaggle.com/code/jeffaudi/aircraft-detection-with-yolov5\">notebook on aircraft detection</a>. If we let overlap, we will have a leak between the training and the validation split which will impact training.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VApbjwkFsXV8"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import tqdm.notebook\n",
        "import copy\n",
        "import PIL\n",
        "\n",
        "# Create 512x512 tiles with 0 pix overlap in df\n",
        "TILE_WIDTH = 512\n",
        "TILE_HEIGHT = 512\n",
        "TILE_OVERLAP = 0 # for now since splitting by image needs to be implemented in IceVision 64\n",
        "TRUNCATED_PERCENT = 0.30\n",
        "_overwriteFiles = True\n",
        "\n",
        "# Location for tiles\n",
        "TILES_PATH = Path('./tiles/')\n",
        "if not os.path.isdir(TILES_PATH):\n",
        "    os.makedirs(TILES_PATH)\n",
        "\n",
        "# check if annotation should be kept\n",
        "def tag_is_inside_tile(bounds, x_start, y_start, width, height, truncated_percent):\n",
        "    x_min, y_min, x_max, y_max = bounds\n",
        "    x_min, y_min, x_max, y_max = x_min - x_start, y_min - y_start, x_max - x_start, y_max - y_start\n",
        "\n",
        "    if (x_min > width) or (x_max < 0.0) or (y_min > height) or (y_max < 0.0):\n",
        "        return None\n",
        "    \n",
        "    x_max_trunc = min(x_max, width) \n",
        "    x_min_trunc = max(x_min, 0) \n",
        "    if (x_max_trunc - x_min_trunc) / (x_max - x_min) < truncated_percent:\n",
        "        return None\n",
        "\n",
        "    y_max_trunc = min(y_max, width) \n",
        "    y_min_trunc = max(y_min, 0) \n",
        "    if (y_max_trunc - y_min_trunc) / (y_max - y_min) < truncated_percent:\n",
        "        return None\n",
        "    \n",
        "    return (x_min_trunc, y_min_trunc, x_max_trunc, y_max_trunc)\n",
        "            \n",
        "rows = []\n",
        "for img_path in tqdm.notebook.tqdm(img_list):\n",
        "    # Open image and related data\n",
        "    pil_img = PIL.Image.open(img_path, mode='r')\n",
        "    np_img = np.array(pil_img, dtype=np.uint8)\n",
        "    image_width, image_height = pil_img.size\n",
        "\n",
        "    # Get annotations for image\n",
        "    img_labels = df[df['image_id'] == img_path.name]\n",
        "    #print(img_labels)\n",
        "\n",
        "    # Count number of sections to make\n",
        "    X_TILES = (image_width + TILE_WIDTH - TILE_OVERLAP - 1) // (TILE_WIDTH - TILE_OVERLAP)\n",
        "    Y_TILES = (image_height + TILE_HEIGHT - TILE_OVERLAP - 1) // (TILE_HEIGHT - TILE_OVERLAP)\n",
        "\n",
        "    # Cut each tile\n",
        "    for x in range(X_TILES):\n",
        "        for y in range(Y_TILES):\n",
        "\n",
        "            x_end = min(TILE_WIDTH + x * (TILE_WIDTH - TILE_OVERLAP), image_width)\n",
        "            x_start = x_end - TILE_WIDTH\n",
        "            y_end = min(TILE_HEIGHT + y * (TILE_HEIGHT - TILE_OVERLAP), image_height)\n",
        "            y_start = y_end - TILE_HEIGHT\n",
        "            #print(x_start, y_start)\n",
        "            \n",
        "            # Save if file doesn't exit\n",
        "            tile_id = img_path.stem + \"_\" + str(x_start) + \"_\" + str(y_start) + img_path.suffix\n",
        "            save_tile_path = TILES_PATH / tile_id\n",
        "            if _overwriteFiles or not os.path.isfile(save_tile_path):\n",
        "                cut_tile = np.zeros(shape=(TILE_WIDTH, TILE_HEIGHT, 3), dtype=np.uint8)\n",
        "                cut_tile[0:TILE_HEIGHT, 0:TILE_WIDTH, :] = np_img[y_start:y_end, x_start:x_end, :]\n",
        "                cut_tile_img = PIL.Image.fromarray(cut_tile)\n",
        "                cut_tile_img.save(save_tile_path)\n",
        "\n",
        "            # Get annotations in tile\n",
        "            for index, tag in img_labels.iterrows():\n",
        "                bounds = tag_is_inside_tile(tag['bounds'], x_start, y_start, TILE_WIDTH, TILE_HEIGHT, TRUNCATED_PERCENT)\n",
        "                if bounds is not None:\n",
        "                    x_min, y_min, x_max, y_max = bounds\n",
        "                    row = {\n",
        "                        'image_id': img_path.name,\n",
        "                        'tile_id': tile_id,\n",
        "                        'x_start': x_start,\n",
        "                        'y_start': y_start,\n",
        "                        'x_min': x_min,\n",
        "                        'x_max': x_max,\n",
        "                        'y_min': y_min,\n",
        "                        'y_max': y_max,\n",
        "                        'label': tag['label'],\n",
        "                    }\n",
        "                    rows.append(copy.deepcopy(row))\n",
        "\n",
        "tiles_df = pd.DataFrame(rows)\n",
        "tiles_df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzTU5yXGsXV8"
      },
      "source": [
        "Next, we need to write an IceVision `Parser`. This is one of the most magical piece of code in IceVision. It enables to smoothly use our content in `PyTorch Lightning` and `Fastai` DataLoaders. Here is the functions wthat we need to implement to create an IceVision `Parser`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HcKavjQsXWB"
      },
      "outputs": [],
      "source": [
        "TEMPLATE_RECORD = ObjectDetectionRecord()\n",
        "Parser.generate_template(TEMPLATE_RECORD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDjoNuwXsXWB"
      },
      "source": [
        "Hopefully, if you have a Pandas `DataFrame`, it is pretty straightfoward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n5eib2HsXWB"
      },
      "outputs": [],
      "source": [
        "class AirbusAircraftParser(Parser):\n",
        "    def __init__(self, template_record, df):\n",
        "        super().__init__(template_record=template_record)\n",
        "\n",
        "        self.df = df\n",
        "        self.class_map = ClassMap(list(self.df['label'].unique()))\n",
        "\n",
        "    def __iter__(self) -> Any:\n",
        "        for o in self.df.itertuples():\n",
        "            yield o\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def record_id(self, o) -> Hashable:\n",
        "        return o.tile_id\n",
        "\n",
        "    def parse_fields(self, o, record, is_new):\n",
        "        if is_new:\n",
        "            record.set_filepath(TILES_PATH / o.tile_id)\n",
        "            record.set_img_size(ImgSize(width=TILE_WIDTH, height=TILE_HEIGHT))\n",
        "            record.detection.set_class_map(self.class_map)\n",
        "\n",
        "        record.detection.add_bboxes([BBox.from_xyxy(o.x_min, o.y_min, o.x_max, o.y_max)])\n",
        "        record.detection.add_labels([o.label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9jzLV0EsXWC"
      },
      "outputs": [],
      "source": [
        "# here we create the parser for the Airbus aircraft dataset\n",
        "parser = AirbusAircraftParser(TEMPLATE_RECORD, tiles_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG4BprFdsXXc"
      },
      "outputs": [],
      "source": [
        "# we check the number and name of classes\n",
        "print(parser.class_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQpF5TwJsXXd"
      },
      "outputs": [],
      "source": [
        "# here, parser takes care of splitting train/valid \n",
        "# we define the seed to have consistent train/valid between runs\n",
        "# parser also correcting/removing records which are incorrects\n",
        "seed_everything(SEED)\n",
        "train_records, valid_records = parser.parse(RandomSplitter([0.8, 0.2], seed=SEED))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8cNdCAXsXXd"
      },
      "outputs": [],
      "source": [
        "# let's display some records!\n",
        "show_records(random.choices(train_records, k=6), ncols=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6858AKlBsXX7"
      },
      "source": [
        "### Transformations\n",
        "\n",
        "The purpose of using transformations is to programmatically increase the number of images used to train the network. Too few images will cause the model to overfit quickly (i.e. learn to replicate exactly the training data). By applying transformations to the training data, we ensure that we can use larger backbones and longer training time â€” globally improving the models while avoiding overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBSYRiBfsXX-"
      },
      "outputs": [],
      "source": [
        "# seed everything\n",
        "seed_everything(SEED)\n",
        "\n",
        "# define some transformation adapted to satellite imagery\n",
        "train_tfms = tfms.A.Adapter([\n",
        "    tfms.A.VerticalFlip(p=0.5),\n",
        "    tfms.A.HorizontalFlip(p=0.5),\n",
        "    tfms.A.Rotate(limit=20),\n",
        "    tfms.A.GaussNoise(p=0.2),\n",
        "    tfms.A.RandomBrightnessContrast(p=0.2),\n",
        "    tfms.A.Normalize(),\n",
        "])\n",
        "\n",
        "# no transformation on the validation split\n",
        "valid_tfms = tfms.A.Adapter([tfms.A.Normalize()])\n",
        "\n",
        "# this is the size of the image after transformations are applied\n",
        "image_size = TILE_WIDTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okHysKxmsXX_"
      },
      "outputs": [],
      "source": [
        "# we create the train/valid Dataset objects\n",
        "train_ds = Dataset(train_records, train_tfms)\n",
        "valid_ds = Dataset(valid_records, valid_tfms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLk0U4qwsXX5"
      },
      "source": [
        "### Model architecture\n",
        "\n",
        "**And this is where the magic happens!** By leveraging the IceVision integration layers, we are able to access easily to hundreds of neural net models.\n",
        "\n",
        "In the notebook, I will test 4 differents models with different backbones\n",
        "- RetinaNet (with ResNet34 or ResNet50 backbones)\n",
        "- FasterRCNN (with ResNet34 or ResNet50 backbones)\n",
        "- EfficientDet (with two small backbones)\n",
        "- YOLOv5 (with medium backbone - similar in size to previous backbones)\n",
        "\n",
        "**Relaunch all cells below when changing the SELECTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjVkJ42QsXX6"
      },
      "outputs": [],
      "source": [
        "# the selected architecture\n",
        "SELECTION = 3\n",
        "\n",
        "# default parameters\n",
        "BATCH_SIZE = 16\n",
        "extra_args = {}\n",
        "\n",
        "if SELECTION == 1:\n",
        "    model_name = \"torchvision-retinanet-resnet34_fpn\"\n",
        "\n",
        "elif SELECTION == 2:\n",
        "    model_name = \"torchvision-retinanet-resnet50_fpn\"\n",
        "    \n",
        "elif SELECTION == 3:\n",
        "    model_name = \"torchvision-faster_rcnn-resnet34_fpn\"\n",
        "\n",
        "elif SELECTION == 4:\n",
        "    model_name = \"torchvision-faster_rcnn-resnet50_fpn\"\n",
        "    \n",
        "elif SELECTION == 5:\n",
        "    model_name = \"ross-efficientdet-tf_lite0\"\n",
        "    extra_args['img_size'] = image_size\n",
        "    \n",
        "elif SELECTION == 6:\n",
        "    model_name = \"ross-efficientdet-d0\"\n",
        "    extra_args['img_size'] = image_size\n",
        "    \n",
        "elif SELECTION == 7:\n",
        "    model_name = \"ultralytics-yolov5-medium\"\n",
        "    extra_args['img_size'] = image_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTjs7O87sXX6"
      },
      "outputs": [],
      "source": [
        "# seed everything\n",
        "seed_everything(SEED)\n",
        "\n",
        "tokens = model_name.split(\"-\")\n",
        "library_name = tokens[-3]\n",
        "print(f\"Library name: {library_name}\")\n",
        "arch_name = tokens[-2]\n",
        "print(f\"Architecture name: {arch_name}\")\n",
        "backbone_name = tokens[-1]\n",
        "print(f\"Backbone name: {backbone_name}\")\n",
        "\n",
        "model_type = getattr(getattr(models, library_name), arch_name)\n",
        "backbone = getattr(model_type.backbones, backbone_name)\n",
        "\n",
        "model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), **extra_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDQmfLOtsXX_"
      },
      "outputs": [],
      "source": [
        "# we create the train/valid DataLoaders objects\n",
        "train_dl = model_type.train_dl(train_ds, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
        "valid_dl = model_type.valid_dl(valid_ds, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-d4KPLasXYC"
      },
      "outputs": [],
      "source": [
        "# and display the first batch\n",
        "model_type.show_batch(first(valid_dl), ncols=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I38oh6UasXYC"
      },
      "outputs": [],
      "source": [
        "# define the metric\n",
        "metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDjInrhZcSJp"
      },
      "source": [
        "## Finding LR with Fastai\n",
        "You can uncomment the following two cells to use Fastai to find the best learning rate for each architecture.\n",
        "\n",
        "*Note: when you are done with this part, comment the code again and rerun the previous cells from the architecture SELECTION before moving on to the next part.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whPwzL8AcSJq"
      },
      "outputs": [],
      "source": [
        "#seed_everything(SEED)\n",
        "#learn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMiuTGzocSJq"
      },
      "outputs": [],
      "source": [
        "#learn.lr_find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGbZwW8TcSJq"
      },
      "outputs": [],
      "source": [
        "if SELECTION == 1 or SELECTION == 3:\n",
        "    LR = 1e-4\n",
        "elif SELECTION == 2 or SELECTION == 4:\n",
        "    LR = 5e-5\n",
        "else:\n",
        "    LR = 0.001\n",
        "    \n",
        "print(f\"Model is {model_name}\")\n",
        "print(f\"Learning rate is {LR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyWZyr1lsXYC"
      },
      "source": [
        "## Training using PyTorch Lightning\n",
        "\n",
        "For the sake of demonstration, we will now use PyTorch Lightning to train our models. We can move easily from one library to another depending on our needs.\n",
        "\n",
        "This is the minimum code that we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrLK4Kt_sXYC"
      },
      "outputs": [],
      "source": [
        "class LightModel(model_type.lightning.ModelAdapter):\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=LR)\n",
        "\n",
        "light_model = LightModel(model, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N76FwG8vcSJr"
      },
      "outputs": [],
      "source": [
        "seed_everything(SEED)\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "trainer = pl.Trainer(max_epochs=MAX_EPOCHS, accelerator='gpu', devices=1, log_every_n_steps=10)\n",
        "trainer.fit(light_model, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the final metric\n",
        "To compute the final metric, we will create a new PyTorch Lightning `trainer`\n",
        " to make sure that we have a clean state and use the `test()` function."
      ],
      "metadata": {
        "id": "ruXyM1cuYfDX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MooR50IssXYF"
      },
      "outputs": [],
      "source": [
        "# compute the final metric with a new Trainer object\n",
        "seed_everything(SEED)\n",
        "valid_dl = model_type.valid_dl(valid_ds, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
        "trainer = pl.Trainer(accelerator='gpu', devices=1)\n",
        "results = trainer.test(light_model, valid_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbpZ9JJicSJs"
      },
      "outputs": [],
      "source": [
        "# print architecture, loss and metric\n",
        "loss = results[0]['test_loss']\n",
        "metric = results[0]['COCOMetric']\n",
        "print(f\"| {model_name} | {loss} | {metric} |\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU216reBsXYF"
      },
      "source": [
        "Accumulating the results in this table\n",
        "\n",
        "| Architecture | Best validation loss | Best COCOMetric |\n",
        "| --- | --- | --- |\n",
        "| torchvision-retinanet-resnet34_fpn | 0.46982264518737793 | 0.49574658971328095 |\n",
        "| torchvision-retinanet-resnet34_fpn | 0.46982264518737793 | 0.49574658971328095 |\n",
        "| torchvision-retinanet-resnet50_fpn | 0.3995179533958435 | 0.5556400150095998 |\n",
        "| torchvision-retinanet-resnet50_fpn | 0.3995179533958435 | 0.5556400150095998 |\n",
        "| torchvision-faster_rcnn-resnet18_fpn | 0.8484785556793213 | 0.1651134579186384 |\n",
        "| torchvision-faster_rcnn-resnet34_fpn | 0.24665088951587677 | 0.5957194089424052 |\n",
        "| torchvision-faster_rcnn-resnet34_fpn | 0.2362484633922577 | 0.6065852414795327 |\n",
        "| torchvision-faster_rcnn-resnet34_fpn | 0.2500587999820709 | 0.5787572670787772 |\n",
        "| torchvision-faster_rcnn-resnet50_fpn | 0.2642669975757599 | 0.5958341939769323 |\n",
        "| torchvision-faster_rcnn-resnet50_fpn | 0.26830238103866577 | 0.5765243207445334 |\n",
        "| ross-efficientdet-tf_lite0 | 0.30016225576400757 | 0.6021635117326537 |\n",
        "| ross-efficientdet-tf_lite0 | 0.30016225576400757 | 0.6021635117326537 |\n",
        "| ross-efficientdet-d0 | 0.29007476568222046 | 0.6077941592823384 |\n",
        "| ross-efficientdet-d0 | 0.29007476568222046 | 0.6077941592823384 |\n",
        "| ultralytics-yolov5-medium | 0.3424062132835388 | 0.5123324077224909 |\n",
        "| ultralytics-yolov5-medium | 0.508611261844635 | 0.23745982558895537 |\n",
        "| ultralytics-yolov5-medium | 0.42536768317222595 | 0.3929547212353217 |\n",
        "| ultralytics-yolov5-medium | 0.4761926829814911 | 0.3119549499080895 |\n",
        "| ultralytics-yolov5-medium | 0.44334524869918823 | 0.34793269230592194 |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the detections by using the following code:"
      ],
      "metadata": {
        "id": "-P82VjUWY-Vl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz3rY6frcSJs"
      },
      "outputs": [],
      "source": [
        "model_type.show_results(model, valid_ds, detection_threshold=.30)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better understand the behavior of our model, we can visualize the results by using the following code:"
      ],
      "metadata": {
        "id": "G5DqPhprZEZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAJ8TmzjcSJs"
      },
      "outputs": [],
      "source": [
        "samples_plus_losses, preds, losses_stats = model_type.interp.plot_top_losses(model=model, dataset=valid_ds, sort_by=\"loss_total\", n_samples=6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thank you for reading till the end :)\n",
        "\n",
        "Please, check my other notebooks:\n",
        "- https://www.kaggle.com/code/jeffaudi/aircraft-detection-with-yolov5\n",
        "- https://www.kaggle.com/code/jeffaudi/eda-airbus-oil-storage-tanks-dataset\n",
        "- https://www.kaggle.com/code/jeffaudi/oil-storage-detection-on-airbus-imagery-with-yolox"
      ],
      "metadata": {
        "id": "7YRo4ShRWB_q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X8sH2mUyZOG_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}